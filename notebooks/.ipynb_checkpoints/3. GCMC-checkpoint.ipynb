{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5e4739-89a9-49fa-9e2e-5db36b4b7745",
   "metadata": {},
   "source": [
    "## Introduction and short model overview\n",
    "Here I will be implementing the GCMC (Graph convolutional matrix completion) model from [this paper](https://arxiv.org/pdf/1706.02263.pdf).\\\n",
    "Some hints and ideas are taken from [here](https://github.com/YuxuanLongBeyond/Graph-based-Recommendation-System/blob/master/scripts/)\\\n",
    "The model has the following structure:\\\n",
    "![](./../reports/figures/model_overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4096c-9ad5-43d7-999f-953ffca65da6",
   "metadata": {},
   "source": [
    "Initially we have the bipartite graph of users and items. We use this bipartite graph to obtain embeddings for users and items (not the ones that we have in the data like genres. Those embeddings will be included later) using message passing. Then, using those embeddings we use bilinear decoder to create confidence map for every rating (so map for rating 1, different map for rating 2 and so on.). Using those maps we choose the rating of item for some user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e655d074-2ee9-4dde-88bc-37ffcc9c954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.sparse as sp\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86e23a7-37f7-4973-905e-bb216f2521b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924b4067-5edb-4676-b87f-e7489bfca173",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ON_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb76c8f-8817-468e-aa12-728ba944fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_ON_GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e0eba-a841-4aab-bb66-d830760b6757",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "All 3 are tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be055341-eb3f-4323-809a-a3340db1168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = torch.load(\"./../data/interim/ratings.pt\").float()\n",
    "items = torch.load(\"./../data/interim/movies.pt\").float() # Contains only information about genre \n",
    "users = torch.load(\"./../data/interim/users.pt\").float() # info about gender, age, and occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30191908-9111-49d9-8610-0bbcc723db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_items = ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154896a-8108-45b4-8a9f-55268c14ff6d",
   "metadata": {},
   "source": [
    "## Splitting the ratings into train and test.\n",
    "We create a mask over existing ratings and divide this mask into train mask and test mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c039b2e-780d-4258-8699-91d26309def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8 # 80% for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e22913-029f-4aa7-88ed-e9e89a8fa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ratings > 0 \n",
    "mask_new = mask + np.random.uniform(0, 1, (num_users, num_items))\n",
    "train_mask = (mask_new <= (1 + split_ratio)) & mask\n",
    "test_mask = (mask_new > (1 + split_ratio)) & mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ded647-b140-450f-abde-08fd97463923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2009)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that we are ok\n",
    "test_mask.sum()/(test_mask.sum() + train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7832f989-25d8-4dcd-87eb-51b54f8eda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide our rating tensor into train and test matrices\n",
    "\n",
    "ratings_train = ratings.clone().detach()\n",
    "ratings_train[test_mask] = 0\n",
    "\n",
    "ratings_test = ratings.clone().detach()\n",
    "ratings_test[train_mask] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327eb4dd-4abd-49ef-a797-f8079d589be1",
   "metadata": {},
   "source": [
    "## Here we define the matrices M_r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80197764-44b8-408c-8117-51f50e79fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(M):\n",
    "    '''\n",
    "    Function to normalize M_r. Remember, when calculating H we have left-multiplication by D^-1, that is exactly the thing we do here\n",
    "    '''\n",
    "    s = torch.sum(M, axis = 1)\n",
    "    s[s == 0] = 1\n",
    "    return (M.T / s).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6d11bc-ccb8-45e2-bf8b-b7fa81b9fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_M_u = []\n",
    "all_M_v = []\n",
    "all_M = [] # For Loss\n",
    "for i in range(5): # We have 1,2,3,4,5 ratings\n",
    "    # Shows which elements have rating i+1\n",
    "    M_r = ratings_train == (i + 1) # because start from 0, but ratings from 1\n",
    "\n",
    "    \n",
    "    all_M_u.append(normalize(M_r))\n",
    "    all_M_v.append(normalize(M_r.T))\n",
    "    all_M.append(M_r.float())\n",
    "    \n",
    "mask = ratings_train > 0   # for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "038f3dc9-54bc-468d-86ea-086c2b569572",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_M = torch.stack(all_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fe412-3c08-43d2-98d2-698f446298e5",
   "metadata": {},
   "source": [
    "## Here we define initial embeddings according to the paper.\n",
    "Remember, the stack of features should give identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f16d68ee-c313-4067-8e7f-babbe1134a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### input feature generation\n",
    "feature_dim = num_users + num_items\n",
    "I = torch.eye(num_users + num_items)\n",
    "feature_u = I[:num_users, :] \n",
    "feature_v = I[num_users :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99186aff-599b-4130-8784-a4f01675056b",
   "metadata": {},
   "source": [
    "## Now lets define the model\n",
    "Once again, the model is the encoder and decoder together. Their description can be seen in the report or original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf4ebd2-53c2-4f7f-938e-ab056cd3caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_drop(feature, drop_out):\n",
    "    '''\n",
    "    Node dropout\n",
    "    drop_out - probability of dropping the output messages of node\n",
    "    '''\n",
    "    tem = torch.rand((feature._nnz())) # nnz is number of non zero elements\n",
    "    feature._values()[tem < drop_out] = 0\n",
    "    return feature\n",
    "\n",
    "class GCMC(nn.Module):\n",
    "    def __init__(self, feature_u, \n",
    "                 feature_v, \n",
    "                 feature_dim, \n",
    "                 hidden_dim, \n",
    "                 rate_num, \n",
    "                 all_M_u, \n",
    "                 all_M_v, \n",
    "                 side_hidden_dim, \n",
    "                 side_feature_u, \n",
    "                 side_feature_v, \n",
    "                 out_dim, \n",
    "                 drop_out = 0.0):\n",
    "        super(GCMC, self).__init__()\n",
    "        \n",
    "        self.drop_out = drop_out\n",
    "        \n",
    "        side_feature_u_dim = side_feature_u.shape[1]\n",
    "        side_feature_v_dim = side_feature_v.shape[1]\n",
    "\n",
    "        self.feature_u = feature_u\n",
    "        self.feature_v = feature_v\n",
    "        self.rate_num = rate_num\n",
    "        \n",
    "        self.num_user = feature_u.shape[0]\n",
    "        self.num_item = feature_v.shape[1]\n",
    "        \n",
    "        self.side_feature_u = side_feature_u\n",
    "        self.side_feature_v = side_feature_v\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(rate_num, feature_dim, hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.W, mode = 'fan_out', nonlinearity = 'relu')\n",
    "        \n",
    "        self.all_M_u = all_M_u\n",
    "        self.all_M_v = all_M_v\n",
    "        \n",
    "        self.reLU = nn.ReLU()\n",
    "\n",
    "\n",
    "        \n",
    "        # Side features tranaformations \n",
    "        self.linear_layer_side_u = nn.Sequential(*[nn.Linear(side_feature_u_dim, side_hidden_dim, bias = True), \n",
    "                                                    nn.BatchNorm1d(side_hidden_dim), nn.ReLU()])\n",
    "        self.linear_layer_side_v = nn.Sequential(*[nn.Linear(side_feature_v_dim, side_hidden_dim, bias = True), \n",
    "                                                    nn.BatchNorm1d(side_hidden_dim), nn.ReLU()])\n",
    "        \n",
    "        \n",
    "        # transformations of final embeddings\n",
    "        self.linear_cat_u = nn.Sequential(*[nn.Linear(rate_num * hidden_dim * 2 + side_hidden_dim, out_dim, bias = True), \n",
    "                                            nn.BatchNorm1d(out_dim), nn.ReLU()])\n",
    "        self.linear_cat_v = nn.Sequential(*[nn.Linear(rate_num * hidden_dim * 2 + side_hidden_dim, out_dim, bias = True), \n",
    "                                            nn.BatchNorm1d(out_dim), nn.ReLU()])   \n",
    "\n",
    "        \n",
    "        # for decoder\n",
    "        self.Q = nn.Parameter(torch.randn(rate_num, out_dim, out_dim))\n",
    "        nn.init.orthogonal_(self.Q)\n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "\n",
    "        # Here is the node drop + normalization to have no problems with mean\n",
    "        feature_u_drop = sparse_drop(self.feature_u, self.drop_out) / (1.0 - self.drop_out)\n",
    "        feature_v_drop = sparse_drop(self.feature_v, self.drop_out) / (1.0 - self.drop_out)\n",
    "        \n",
    "        hidden_feature_u = []\n",
    "        hidden_feature_v = []\n",
    "        \n",
    "        W_list = torch.split(self.W, self.rate_num) \n",
    "        W_flat = []\n",
    "        for i in range(self.rate_num): # iterate over every rating\n",
    "            Wr = W_list[0][i]\n",
    "            \n",
    "            M_u = self.all_M_u[i]\n",
    "            M_v = self.all_M_v[i] # Just M_u transposed\n",
    "            \n",
    "            # H_u from paper. The embeddings\n",
    "            hidden_u = sp.mm(feature_v_drop, Wr)\n",
    "            hidden_u = self.reLU(sp.mm(M_u, hidden_u))\n",
    "\n",
    "            # H_v\n",
    "            hidden_v = sp.mm(feature_u_drop, Wr)\n",
    "            hidden_v = self.reLU(sp.mm(M_v, hidden_v))\n",
    "\n",
    "            \n",
    "            hidden_feature_u.append(hidden_u)\n",
    "            hidden_feature_v.append(hidden_v)\n",
    "            \n",
    "            W_flat.append(Wr)\n",
    "\n",
    "        \n",
    "        hidden_feature_u = torch.cat(hidden_feature_u, dim = 1)\n",
    "        hidden_feature_v = torch.cat(hidden_feature_v, dim = 1)\n",
    "        # Now we have H_u and H_v. Note that there is no non-linearity at the end, because we did ReLU before and ReLU can be done separately.\n",
    "        W_flat = torch.cat(W_flat, dim = 1) \n",
    "\n",
    "        # Here we add self-messages\n",
    "        cat_u = torch.cat((hidden_feature_u, torch.mm(self.feature_u, W_flat)), dim = 1)\n",
    "        cat_v = torch.cat((hidden_feature_v, torch.mm(self.feature_v, W_flat)), dim = 1)\n",
    "\n",
    "        # Here we transform side-featurs and add them to our embeddings\n",
    "        side_hidden_feature_u = self.linear_layer_side_u(self.side_feature_u)\n",
    "        side_hidden_feature_v = self.linear_layer_side_v(self.side_feature_v)    \n",
    "        cat_u = torch.cat((cat_u, side_hidden_feature_u), dim = 1)\n",
    "        cat_v = torch.cat((cat_v, side_hidden_feature_v), dim = 1)\n",
    "        \n",
    "        # Final embeddings\n",
    "        embed_u = self.linear_cat_u(cat_u)\n",
    "        embed_v = self.linear_cat_v(cat_v)\n",
    "\n",
    "        \n",
    "        # Decoder part -------------------------\n",
    "        \n",
    "        score = [] # Confidence map\n",
    "        \n",
    "        Q_list = torch.split(self.Q, self.rate_num)\n",
    "        for i in range(self.rate_num):\n",
    "            Qr = Q_list[0][i]\n",
    "            tem = torch.mm(torch.mm(embed_u, Qr), torch.t(embed_v))\n",
    "            \n",
    "            score.append(tem)\n",
    "\n",
    "        score = torch.stack(score)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21033dec-2f7f-4b19-9478-83865efe373d",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87439857-e759-4330-9f22-56877185afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse(x):\n",
    "    '''\n",
    "    Function to convert dense tensor x to sparse tensor\n",
    "    \n",
    "    '''\n",
    "    # Save the original type\n",
    "    x_typename = torch.typename(x).split('.')[-1] # FloatTensor usually\n",
    "    sparse_tensortype = getattr(torch.sparse, x_typename) # torch.sparse.FloatTensor usually\n",
    "\n",
    "    indices = torch.nonzero(x)\n",
    "    if len(indices.shape) == 0: # If all elements are zero, then we return zero sparse tensor \n",
    "        return sparse_tensortype(*x.shape)\n",
    "\n",
    "    # Creating the sparse tensor according to documentation\n",
    "    indices = indices.t()\n",
    "    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "    return sparse_tensortype(indices, values, x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7082c8-1401-4e41-90f1-d7e647376b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(feature_u, feature_v, feature_dim, hidden_dim, rate_num, all_M_u, all_M_v, \n",
    "                 side_hidden_dim, side_feature_u, side_feature_v, out_dim, drop_out = 0.0):\n",
    "    ''' \n",
    "    This function prepares the model. \n",
    "    It 1) converts neccesary tensors to sparse format 2) moves model to cuda 3) returns the model\n",
    "    '''\n",
    "    for i in range(rate_num):\n",
    "        all_M_u[i] = to_sparse(all_M_u[i])\n",
    "        all_M_v[i] = to_sparse(all_M_v[i])\n",
    "    \n",
    "    feature_u = to_sparse(feature_u)\n",
    "    feature_v = to_sparse(feature_v)\n",
    "\n",
    "    net = GCMC(feature_u, feature_v, feature_dim, hidden_dim, rate_num, all_M_u, all_M_v, \n",
    "                 side_hidden_dim, side_feature_u, side_feature_v, out_dim, drop_out)\n",
    "\n",
    "    if RUN_ON_GPU:\n",
    "        print('Moving models to GPU.')\n",
    "        net.cuda()\n",
    "    else:\n",
    "        print('Keeping models on CPU.')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6795efcb-30c3-4666-b6f4-c7cf7d2f978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, all_M, mask, user_item_matrix):\n",
    "            \n",
    "        super(Loss, self).__init__()\n",
    "            \n",
    "        self.all_M = all_M\n",
    "        self.mask = mask\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        \n",
    "        self.rate_num = all_M.shape[0]\n",
    "        self.num = float(mask.sum())\n",
    "        \n",
    "        self.logsm = nn.LogSoftmax(dim = 0)\n",
    "        self.sm = nn.Softmax(dim = 0)\n",
    "        \n",
    "    def cross_entropy(self, score):\n",
    "        l = torch.sum(-self.all_M * self.logsm(score))\n",
    "        return l / self.num\n",
    "    \n",
    "    def rmse(self, score):\n",
    "        score_list = torch.split(self.sm(score), self.rate_num)\n",
    "        total_score = 0\n",
    "        for i in range(self.rate_num):\n",
    "            total_score += (i + 1) * score_list[0][i]\n",
    "        \n",
    "        square_err = torch.pow(total_score * self.mask - self.user_item_matrix, 2)\n",
    "        mse = torch.sum(square_err) / self.num\n",
    "        return torch.sqrt(mse)\n",
    "        \n",
    "    def loss(self, score):\n",
    "        return self.cross_entropy(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3035d-0193-4500-8021-11574a663105",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec4414cb-84bc-4125-8749-1633622c7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(score, rate_num, user_item_matrix_test):\n",
    "    sm = nn.Softmax(dim = 0)\n",
    "    score = sm(score)\n",
    "    score_list = torch.split(score, rate_num)\n",
    "    pred = 0\n",
    "    for i in range(rate_num):\n",
    "        pred += (i + 1) * score_list[0][i]\n",
    "\n",
    "    \n",
    "    test_mask = user_item_matrix_test > 0\n",
    "    square_err = (pred * test_mask - user_item_matrix_test) ** 2\n",
    "    mse = square_err.sum() / test_mask.sum()\n",
    "    test_rmse = torch.sqrt(mse)\n",
    "    \n",
    "    return test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "566c30c8-e7ef-47d6-940a-0da97c42f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    rate_num,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    num_epochs,\n",
    "    hidden_dim,\n",
    "    side_hidden_dim,\n",
    "    out_dim,\n",
    "    drop_out,\n",
    "    saved_model_folder,\n",
    "    log_dir\n",
    "):\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    post_fix = '/best_model.pt'\n",
    "    \n",
    "    if not os.path.exists(saved_model_folder):\n",
    "        os.makedirs(saved_model_folder)  \n",
    "    \n",
    "    weights_name = saved_model_folder + post_fix\n",
    "\n",
    "\n",
    "    net = create_model(feature_u, feature_v, feature_dim, hidden_dim, rate_num, all_M_u, all_M_v, \n",
    "                 side_hidden_dim, users, items, out_dim, drop_out)\n",
    "    net.train() # in train mode\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    loss_obj = Loss(all_M, mask, ratings_train)\n",
    "    iter_bar = tqdm(range(num_epochs))\n",
    "\n",
    "    best_val_rmse = 1000000\n",
    "    \n",
    "    for epoch in iter_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        score = net.forward() # Here we get the Confidence map WITHOUT softmax at the end\n",
    "        loss = loss_obj.loss(score) # cross entropy \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rmse = loss_obj.rmse(score)\n",
    "            \n",
    "            val_rmse = validate(score, rate_num, ratings_test)\n",
    "            iter_bar.set_description('Iter (loss=%5.3f, rmse=%5.3f, val_rmse=%5.5f)'%(loss.item(),rmse.item(), val_rmse.item()))\n",
    "            \n",
    "            writer.add_scalar(\"RMSE/test\", val_rmse.item(), epoch)\n",
    "            writer.add_scalar(\"RMSE/train\", rmse.item(), epoch)\n",
    "            \n",
    "            if (val_rmse.item() < best_val_rmse) and (epoch > 150):\n",
    "                torch.save(net, weights_name)\n",
    "                best_val_rmse = val_rmse.item()\n",
    "    print('Best Validation RMSE: ', best_val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5139f0a-e27b-4a0f-8bfe-405194a12a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\porlu\\AppData\\Local\\Temp\\ipykernel_9944\\2748016457.py:17: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:607.)\n",
      "  return sparse_tensortype(indices, values, x.size())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping models on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=1.101, rmse=0.844, val_rmse=0.91699): 100%|███████████████████████████████| 500/500 [01:16<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation RMSE:  0.9162934422492981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main(rate_num=5, \n",
    "     lr = 1e-2,\n",
    "     weight_decay=0.005, \n",
    "     num_epochs=500,\n",
    "     hidden_dim=5, \n",
    "     side_hidden_dim=5, \n",
    "     out_dim=5, \n",
    "     drop_out=0.0,\n",
    "     saved_model_folder=\"./../models\",\n",
    "     log_dir = \"./../tensorboard_logs\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
