{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5e4739-89a9-49fa-9e2e-5db36b4b7745",
   "metadata": {},
   "source": [
    "## Introduction and short model overview\n",
    "Here I will be implementing the GCMC (Graph convolutional matrix completion) model from [this paper](https://arxiv.org/pdf/1706.02263.pdf).\\\n",
    "Some hints and ideas are taken from [here](https://github.com/YuxuanLongBeyond/Graph-based-Recommendation-System/blob/master/scripts/)\\\n",
    "The model has the following structure:\\\n",
    "![](./../reports/figures/model_overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4096c-9ad5-43d7-999f-953ffca65da6",
   "metadata": {},
   "source": [
    "Initially we have the bipartite graph of users and items. We use this bipartite graph to obtain embeddings for users and items (not the ones that we have in the data like genres. Those embeddings will be included later) using message passing. Then, using those embeddings we use bilinear decoder to create confidence map for every rating (so map for rating 1, different map for rating 2 and so on.). Using those maps we choose the rating of item for some user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e655d074-2ee9-4dde-88bc-37ffcc9c954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.sparse as sp\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c86e23a7-37f7-4973-905e-bb216f2521b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924b4067-5edb-4676-b87f-e7489bfca173",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ON_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb76c8f-8817-468e-aa12-728ba944fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_ON_GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e0eba-a841-4aab-bb66-d830760b6757",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "All 3 are tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "be055341-eb3f-4323-809a-a3340db1168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = torch.load(\"./../data/interim/ratings.pt\").float()\n",
    "items = torch.load(\"./../data/interim/movies.pt\").float() # Contains only information about genre \n",
    "users = torch.load(\"./../data/interim/users.pt\").float() # info about gender, age, and occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "30191908-9111-49d9-8610-0bbcc723db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_items = ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154896a-8108-45b4-8a9f-55268c14ff6d",
   "metadata": {},
   "source": [
    "## Splitting the ratings into train and test.\n",
    "We create a mask over existing ratings and divide this mask into train mask and test mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c039b2e-780d-4258-8699-91d26309def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8 # 80% for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2e22913-029f-4aa7-88ed-e9e89a8fa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ratings > 0 \n",
    "mask_new = mask + np.random.uniform(0, 1, (num_users, num_items))\n",
    "train_mask = (mask_new <= (1 + split_ratio)) & mask\n",
    "test_mask = (mask_new > (1 + split_ratio)) & mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20ded647-b140-450f-abde-08fd97463923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2004)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that we are ok\n",
    "test_mask.sum()/(test_mask.sum() + train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7832f989-25d8-4dcd-87eb-51b54f8eda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide our rating tensor into train and test matrices\n",
    "\n",
    "ratings_train = ratings.clone().detach()\n",
    "ratings_train[test_mask] = 0\n",
    "\n",
    "ratings_test = ratings.clone().detach()\n",
    "ratings_test[train_mask] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327eb4dd-4abd-49ef-a797-f8079d589be1",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80197764-44b8-408c-8117-51f50e79fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(M):\n",
    "    s = torch.sum(M, axis = 1)\n",
    "    s[s == 0] = 1\n",
    "    return (M.T / s).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cf6d11bc-ccb8-45e2-bf8b-b7fa81b9fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_M_u = []\n",
    "all_M_v = []\n",
    "all_M = []\n",
    "for i in range(5): # We have 1,2,3,4,5 ratings\n",
    "    M_r = ratings_train == (i + 1) # because start from 0, but ratings from 1\n",
    "    \n",
    "    all_M_u.append(normalize(M_r))\n",
    "    all_M_v.append(normalize(M_r.T))\n",
    "    all_M.append(M_r.float())\n",
    "mask = ratings_train > 0   # for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "038f3dc9-54bc-468d-86ea-086c2b569572",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_M = torch.stack(all_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f16d68ee-c313-4067-8e7f-babbe1134a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### input feature generation\n",
    "feature_dim = num_users + num_items\n",
    "I = torch.eye(num_users + num_items)\n",
    "feature_u = I[:num_users, :] \n",
    "feature_v = I[num_users :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99186aff-599b-4130-8784-a4f01675056b",
   "metadata": {},
   "source": [
    "## Now lets define the model\n",
    "Once again, the model is the encoder and decoder together. Their description can be seen in the report or original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cbf4ebd2-53c2-4f7f-938e-ab056cd3caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_drop(feature, drop_out):\n",
    "    tem = torch.rand((feature._nnz())) # nnz is number of non zero elements\n",
    "    feature._values()[tem < drop_out] = 0\n",
    "    return feature\n",
    "\n",
    "class GCMC(nn.Module):\n",
    "    def __init__(self, feature_u, \n",
    "                 feature_v, \n",
    "                 feature_dim, \n",
    "                 hidden_dim, \n",
    "                 rate_num, \n",
    "                 all_M_u, \n",
    "                 all_M_v, \n",
    "                 side_hidden_dim, \n",
    "                 side_feature_u, \n",
    "                 side_feature_v, \n",
    "                 use_side : bool,\n",
    "                 out_dim, \n",
    "                 drop_out = 0.0):\n",
    "        super(GCMC, self).__init__()\n",
    "        \n",
    "        self.drop_out = drop_out\n",
    "        \n",
    "        side_feature_u_dim = side_feature_u.shape[1]\n",
    "        side_feature_v_dim = side_feature_v.shape[1]\n",
    "        self.use_side = use_side\n",
    "\n",
    "        self.feature_u = feature_u\n",
    "        self.feature_v = feature_v\n",
    "        self.rate_num = rate_num\n",
    "        \n",
    "        self.num_user = feature_u.shape[0]\n",
    "        self.num_item = feature_v.shape[1]\n",
    "        \n",
    "        self.side_feature_u = side_feature_u\n",
    "        self.side_feature_v = side_feature_v\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(rate_num, feature_dim, hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.W, mode = 'fan_out', nonlinearity = 'relu')\n",
    "        \n",
    "        self.all_M_u = all_M_u\n",
    "        self.all_M_v = all_M_v\n",
    "        \n",
    "        self.reLU = nn.ReLU()\n",
    "        \n",
    "        if use_side:\n",
    "            self.linear_layer_side_u = nn.Sequential(*[nn.Linear(side_feature_u_dim, side_hidden_dim, bias = True), \n",
    "                                                       nn.BatchNorm1d(side_hidden_dim), nn.ReLU()])\n",
    "            self.linear_layer_side_v = nn.Sequential(*[nn.Linear(side_feature_v_dim, side_hidden_dim, bias = True), \n",
    "                                                       nn.BatchNorm1d(side_hidden_dim), nn.ReLU()])\n",
    "    \n",
    "            self.linear_cat_u = nn.Sequential(*[nn.Linear(rate_num * hidden_dim * 2 + side_hidden_dim, out_dim, bias = True), \n",
    "                                                nn.BatchNorm1d(out_dim), nn.ReLU()])\n",
    "            self.linear_cat_v = nn.Sequential(*[nn.Linear(rate_num * hidden_dim * 2 + side_hidden_dim, out_dim, bias = True), \n",
    "                                                nn.BatchNorm1d(out_dim), nn.ReLU()])    \n",
    "        else:\n",
    "            \n",
    "            self.linear_cat_u = nn.Sequential(*[nn.Linear(rate_num * hidden_dim * 2, out_dim, bias = True), \n",
    "                                                nn.BatchNorm1d(out_dim), nn.ReLU()])\n",
    "            self.linear_cat_v = nn.Sequential(*[nn.Linear(rate_num * hidden_dim * 2, out_dim, bias = True), \n",
    "                                                nn.BatchNorm1d(out_dim), nn.ReLU()])\n",
    "            \n",
    "        self.Q = nn.Parameter(torch.randn(rate_num, out_dim, out_dim))\n",
    "        nn.init.orthogonal_(self.Q)\n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "\n",
    "        # Here is the drop + normalization to have no problems with mean\n",
    "        feature_u_drop = sparse_drop(self.feature_u, self.drop_out) / (1.0 - self.drop_out)\n",
    "        feature_v_drop = sparse_drop(self.feature_v, self.drop_out) / (1.0 - self.drop_out)\n",
    "        \n",
    "        hidden_feature_u = []\n",
    "        hidden_feature_v = []\n",
    "        \n",
    "        W_list = torch.split(self.W, self.rate_num) # we have different W for every rating, as stated in the paper\n",
    "        W_flat = []\n",
    "        for i in range(self.rate_num): # iterate over every rating\n",
    "            Wr = W_list[0][i]\n",
    "            M_u = self.all_M_u[i]\n",
    "            M_v = self.all_M_v[i]\n",
    "            \n",
    "            # H_u from paper. The embeddings\n",
    "            hidden_u = sp.mm(feature_v_drop, Wr)\n",
    "            hidden_u = self.reLU(sp.mm(M_u, hidden_u))\n",
    "            \n",
    "            # need to further process M, normalization\n",
    "            hidden_v = sp.mm(feature_u_drop, Wr)\n",
    "            hidden_v = self.reLU(sp.mm(M_v, hidden_v))\n",
    "\n",
    "            \n",
    "            hidden_feature_u.append(hidden_u)\n",
    "            hidden_feature_v.append(hidden_v)\n",
    "            \n",
    "            W_flat.append(Wr)\n",
    "            \n",
    "        hidden_feature_u = torch.cat(hidden_feature_u, dim = 1)\n",
    "        hidden_feature_v = torch.cat(hidden_feature_v, dim = 1)\n",
    "        W_flat = torch.cat(W_flat, dim = 1)\n",
    "\n",
    "\n",
    "        cat_u = torch.cat((hidden_feature_u, torch.mm(self.feature_u, W_flat)), dim = 1)\n",
    "        cat_v = torch.cat((hidden_feature_v, torch.mm(self.feature_v, W_flat)), dim = 1)\n",
    "        \n",
    "        if self.use_side:\n",
    "            side_hidden_feature_u = self.linear_layer_side_u(self.side_feature_u)\n",
    "            side_hidden_feature_v = self.linear_layer_side_v(self.side_feature_v)    \n",
    "\n",
    "            cat_u = torch.cat((cat_u, side_hidden_feature_u), dim = 1)\n",
    "            cat_v = torch.cat((cat_v, side_hidden_feature_v), dim = 1)\n",
    "        \n",
    "        \n",
    "        embed_u = self.linear_cat_u(cat_u)\n",
    "        embed_v = self.linear_cat_v(cat_v)\n",
    "        \n",
    "        score = []\n",
    "        Q_list = torch.split(self.Q, self.rate_num)\n",
    "        for i in range(self.rate_num):\n",
    "            Qr = Q_list[0][i]\n",
    "            \n",
    "            tem = torch.mm(torch.mm(embed_u, Qr), torch.t(embed_v))\n",
    "            \n",
    "            score.append(tem)\n",
    "            \n",
    "        return torch.stack(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21033dec-2f7f-4b19-9478-83865efe373d",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87439857-e759-4330-9f22-56877185afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse(x):\n",
    "    \"\"\" converts dense tensor x to sparse format \"\"\"\n",
    "    x_typename = torch.typename(x).split('.')[-1] # FloatTensor usually\n",
    "    sparse_tensortype = getattr(torch.sparse, x_typename) # torch.sparse.FloatTensor usually\n",
    "\n",
    "    indices = torch.nonzero(x)\n",
    "    if len(indices.shape) == 0:  # if all elements are zeros\n",
    "        return sparse_tensortype(*x.shape)\n",
    "    indices = indices.t()\n",
    "    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "    return sparse_tensortype(indices, values, x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c7082c8-1401-4e41-90f1-d7e647376b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(feature_u, feature_v, feature_dim, hidden_dim, rate_num, all_M_u, all_M_v, \n",
    "                 side_hidden_dim, side_feature_u, side_feature_v, use_side, out_dim, drop_out = 0.0):\n",
    "    \n",
    "    for i in range(rate_num):\n",
    "        all_M_u[i] = to_sparse(all_M_u[i])\n",
    "        all_M_v[i] = to_sparse(all_M_v[i])\n",
    "    \n",
    "    feature_u = to_sparse(feature_u)\n",
    "    feature_v = to_sparse(feature_v)\n",
    "\n",
    "    net = GCMC(feature_u, feature_v, feature_dim, hidden_dim, rate_num, all_M_u, all_M_v, \n",
    "                 side_hidden_dim, side_feature_u, side_feature_v, use_side, out_dim, drop_out)\n",
    "\n",
    "    if RUN_ON_GPU:\n",
    "        print('Moving models to GPU.')\n",
    "        net.cuda()\n",
    "    else:\n",
    "        print('Keeping models on CPU.')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6795efcb-30c3-4666-b6f4-c7cf7d2f978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, all_M, mask, user_item_matrix):\n",
    "            \n",
    "        super(Loss, self).__init__()\n",
    "            \n",
    "        self.all_M = all_M\n",
    "        self.mask = mask\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        \n",
    "        self.rate_num = all_M.shape[0]\n",
    "        self.num = float(mask.sum())\n",
    "        \n",
    "        self.logsm = nn.LogSoftmax(dim = 0)\n",
    "        self.sm = nn.Softmax(dim = 0)\n",
    "        \n",
    "    def cross_entropy(self, score):\n",
    "        l = torch.sum(-self.all_M * self.logsm(score))\n",
    "        return l / self.num\n",
    "    \n",
    "    def rmse(self, score):\n",
    "        score_list = torch.split(self.sm(score), self.rate_num)\n",
    "        total_score = 0\n",
    "        for i in range(self.rate_num):\n",
    "            total_score += (i + 1) * score_list[0][i]\n",
    "        \n",
    "        square_err = torch.pow(total_score * self.mask - self.user_item_matrix, 2)\n",
    "        mse = torch.sum(square_err) / self.num\n",
    "        return torch.sqrt(mse)\n",
    "        \n",
    "    def loss(self, score):\n",
    "        return self.cross_entropy(score) + self.rmse(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3035d-0193-4500-8021-11574a663105",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec4414cb-84bc-4125-8749-1633622c7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(score, rate_num, user_item_matrix_test):\n",
    "    sm = nn.Softmax(dim = 0)\n",
    "    score = sm(score)\n",
    "    score_list = torch.split(score, rate_num)\n",
    "    pred = 0\n",
    "    for i in range(rate_num):\n",
    "        pred += (i + 1) * score_list[0][i]\n",
    "\n",
    "    \n",
    "    test_mask = user_item_matrix_test > 0\n",
    "    square_err = (pred * test_mask - user_item_matrix_test) ** 2\n",
    "    mse = square_err.sum() / test_mask.sum()\n",
    "    test_rmse = torch.sqrt(mse)\n",
    "    \n",
    "    return test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "566c30c8-e7ef-47d6-940a-0da97c42f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    rate_num,\n",
    "    use_side_feature : bool,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    num_epochs,\n",
    "    hidden_dim,\n",
    "    side_hidden_dim,\n",
    "    out_dim,\n",
    "    drop_out,\n",
    "    saved_model_folder,\n",
    "    log_dir\n",
    "):\n",
    "    # mark and record the training file, save the training arguments for future analysis\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    post_fix = '/best_model_weights'\n",
    "    \n",
    "    if not os.path.exists(saved_model_folder):\n",
    "        os.makedirs(saved_model_folder)  \n",
    "    weights_name = saved_model_folder + post_fix\n",
    "\n",
    "\n",
    "    net = create_model(feature_u, feature_v, feature_dim, hidden_dim, rate_num, all_M_u, all_M_v, \n",
    "                 side_hidden_dim, users, items, use_side_feature, out_dim, drop_out)\n",
    "    net.train() # in train mode\n",
    "\n",
    "    # create AMSGrad optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    loss_obj = Loss(all_M, mask, ratings_train)\n",
    "    iter_bar = tqdm(range(num_epochs), desc='Iter (loss=X.XXX)')\n",
    "\n",
    "    best_val_rmse = 1000000\n",
    "    for epoch in iter_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        score = net.forward() # Here we get the prediction matrix\n",
    "        loss = loss_obj.loss(score) # cross entropy + RMSE\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rmse = loss_obj.rmse(score)\n",
    "            \n",
    "            val_rmse = validate(score, rate_num, ratings_test)\n",
    "            iter_bar.set_description('Iter (loss=%5.3f, rmse=%5.3f, val_rmse=%5.5f)'%(loss.item(),rmse.item(), val_rmse.item()))\n",
    "            \n",
    "            writer.add_scalar(\"RMSE/test\", val_rmse.item(), epoch)\n",
    "            writer.add_scalar(\"RMSE/train\", rmse.item(), epoch)\n",
    "            \n",
    "            if val_rmse.item() < best_val_rmse:\n",
    "                torch.save(net.state_dict(), weights_name)\n",
    "                best_val_rmse = val_rmse.item()\n",
    "    print('Best Validation RMSE: ', best_val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c5139f0a-e27b-4a0f-8bfe-405194a12a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping models on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=2.064, rmse=0.884, val_rmse=0.93883): 100%|███████████████████████████████| 500/500 [02:13<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation RMSE:  0.9376219511032104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main(rate_num=5, \n",
    "     use_side_feature=True, \n",
    "     lr = 1e-2,\n",
    "     weight_decay=0.00001, \n",
    "     num_epochs=500,\n",
    "     hidden_dim=5, \n",
    "     side_hidden_dim=5, \n",
    "     out_dim=5, \n",
    "     drop_out=0.0,\n",
    "     saved_model_folder=\"./../models\",\n",
    "     log_dir = \"./../tensorboard_logs\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
