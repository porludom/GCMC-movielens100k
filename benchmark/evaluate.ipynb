{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbcfd7b5-d3aa-4a00-bd9b-6a5d48ccbb83",
   "metadata": {},
   "source": [
    "## Here we evaluate the trained model on the saved test part from training on MAP@K metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5b4598-13ed-4663-a2de-336fea06c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.sparse as sp\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb9ba3-59fc-4b17-8cc3-0f83f50d808d",
   "metadata": {},
   "source": [
    "## Definition of the model in order to be able to read it from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e492094e-6328-4bd2-9820-5e7dcebe24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_drop(feature, drop_out):\n",
    "    '''\n",
    "    Node dropout\n",
    "    drop_out - probability of dropping the output messages of node\n",
    "    '''\n",
    "    tem = torch.rand((feature._nnz())) # nnz is number of non zero elements\n",
    "    feature._values()[tem < drop_out] = 0\n",
    "    return feature\n",
    "\n",
    "class GCMC(nn.Module):\n",
    "    def __init__(self, feature_u, \n",
    "                 feature_v, \n",
    "                 feature_dim, \n",
    "                 hidden_dim, \n",
    "                 rate_num, \n",
    "                 all_M_u, \n",
    "                 all_M_v, \n",
    "                 side_hidden_dim, \n",
    "                 side_feature_u, \n",
    "                 side_feature_v, \n",
    "                 out_dim, \n",
    "                 drop_out = 0.0):\n",
    "        super(GCMC, self).__init__()\n",
    "        \n",
    "        self.drop_out = drop_out\n",
    "        \n",
    "        side_feature_u_dim = side_feature_u.shape[1]\n",
    "        side_feature_v_dim = side_feature_v.shape[1]\n",
    "\n",
    "        self.feature_u = feature_u\n",
    "        self.feature_v = feature_v\n",
    "        self.rate_num = rate_num\n",
    "        \n",
    "        self.num_user = feature_u.shape[0]\n",
    "        self.num_item = feature_v.shape[1]\n",
    "        \n",
    "        self.side_feature_u = side_feature_u\n",
    "        self.side_feature_v = side_feature_v\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(rate_num, feature_dim, hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.W, mode = 'fan_out', nonlinearity = 'relu')\n",
    "        \n",
    "        self.all_M_u = all_M_u\n",
    "        self.all_M_v = all_M_v\n",
    "        \n",
    "        self.reLU = nn.ReLU()\n",
    "\n",
    "\n",
    "        \n",
    "        # Side features tranaformations \n",
    "        self.linear_layer_side_u = nn.Sequential(*[nn.Linear(side_feature_u_dim, side_hidden_dim, bias = True), \n",
    "                                                    nn.BatchNorm1d(side_hidden_dim), nn.ReLU()])\n",
    "        self.linear_layer_side_v = nn.Sequential(*[nn.Linear(side_feature_v_dim, side_hidden_dim, bias = True), \n",
    "                                                    nn.BatchNorm1d(side_hidden_dim), nn.ReLU()])\n",
    "        \n",
    "        \n",
    "        # transformations of final embeddings\n",
    "        self.linear_cat_u = nn.Sequential(*[nn.Linear(rate_num * hidden_dim * 2 + side_hidden_dim, out_dim, bias = True), \n",
    "                                            nn.BatchNorm1d(out_dim), nn.ReLU()])\n",
    "        self.linear_cat_v = nn.Sequential(*[nn.Linear(rate_num * hidden_dim * 2 + side_hidden_dim, out_dim, bias = True), \n",
    "                                            nn.BatchNorm1d(out_dim), nn.ReLU()])   \n",
    "\n",
    "        \n",
    "        # for decoder\n",
    "        self.Q = nn.Parameter(torch.randn(rate_num, out_dim, out_dim))\n",
    "        nn.init.orthogonal_(self.Q)\n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "\n",
    "        # Here is the node drop + normalization to have no problems with mean\n",
    "        feature_u_drop = sparse_drop(self.feature_u, self.drop_out) / (1.0 - self.drop_out)\n",
    "        feature_v_drop = sparse_drop(self.feature_v, self.drop_out) / (1.0 - self.drop_out)\n",
    "        \n",
    "        hidden_feature_u = []\n",
    "        hidden_feature_v = []\n",
    "        \n",
    "        W_list = torch.split(self.W, self.rate_num) \n",
    "        W_flat = []\n",
    "        for i in range(self.rate_num): # iterate over every rating\n",
    "            Wr = W_list[0][i]\n",
    "            \n",
    "            M_u = self.all_M_u[i]\n",
    "            M_v = self.all_M_v[i] # Just M_u transposed\n",
    "            \n",
    "            # H_u from paper. The embeddings\n",
    "            hidden_u = sp.mm(feature_v_drop, Wr)\n",
    "            hidden_u = self.reLU(sp.mm(M_u, hidden_u))\n",
    "\n",
    "            # H_v\n",
    "            hidden_v = sp.mm(feature_u_drop, Wr)\n",
    "            hidden_v = self.reLU(sp.mm(M_v, hidden_v))\n",
    "\n",
    "            \n",
    "            hidden_feature_u.append(hidden_u)\n",
    "            hidden_feature_v.append(hidden_v)\n",
    "            \n",
    "            W_flat.append(Wr)\n",
    "\n",
    "        \n",
    "        hidden_feature_u = torch.cat(hidden_feature_u, dim = 1)\n",
    "        hidden_feature_v = torch.cat(hidden_feature_v, dim = 1)\n",
    "        # Now we have H_u and H_v. Note that there is no non-linearity at the end, because we did ReLU before and ReLU can be done separately.\n",
    "        W_flat = torch.cat(W_flat, dim = 1) \n",
    "\n",
    "        # Here we add self-messages\n",
    "        cat_u = torch.cat((hidden_feature_u, torch.mm(self.feature_u, W_flat)), dim = 1)\n",
    "        cat_v = torch.cat((hidden_feature_v, torch.mm(self.feature_v, W_flat)), dim = 1)\n",
    "\n",
    "        # Here we transform side-featurs and add them to our embeddings\n",
    "        side_hidden_feature_u = self.linear_layer_side_u(self.side_feature_u)\n",
    "        side_hidden_feature_v = self.linear_layer_side_v(self.side_feature_v)    \n",
    "        cat_u = torch.cat((cat_u, side_hidden_feature_u), dim = 1)\n",
    "        cat_v = torch.cat((cat_v, side_hidden_feature_v), dim = 1)\n",
    "        \n",
    "        # Final embeddings\n",
    "        embed_u = self.linear_cat_u(cat_u)\n",
    "        embed_v = self.linear_cat_v(cat_v)\n",
    "\n",
    "        \n",
    "        # Decoder part -------------------------\n",
    "        \n",
    "        score = [] # Confidence map\n",
    "        \n",
    "        Q_list = torch.split(self.Q, self.rate_num)\n",
    "        for i in range(self.rate_num):\n",
    "            Qr = Q_list[0][i]\n",
    "            tem = torch.mm(torch.mm(embed_u, Qr), torch.t(embed_v))\n",
    "            \n",
    "            score.append(tem)\n",
    "\n",
    "        score = torch.stack(score)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea7e71-3354-497a-aa98-5464226f432f",
   "metadata": {},
   "source": [
    "## Here we read the test data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce7a409-28af-459e-b252-e995a47a6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test = torch.load(\"ratings_test.pt\")\n",
    "test_mask = torch.load(\"test_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8e7ccc-0ae6-4044-9a98-a4a822d9b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_to_matrix(score):\n",
    "    '''\n",
    "    score is the confidence map over the ratings. We convert it to single matrix of ratings by calculating expectation\n",
    "    '''\n",
    "    sm = nn.Softmax(dim = 0)\n",
    "    score = sm(score)\n",
    "    score_list = torch.split(score, 5)\n",
    "    pred = 0\n",
    "    \n",
    "    # math expectation\n",
    "    for i in range(5):\n",
    "        pred += (i + 1) * score_list[0][i]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a9bd9a-75bd-4d87-83e7-2c339c26a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"./../models/best_model.pt\")\n",
    "generated_ratings = scores_to_matrix(net.forward())\n",
    "generated_ratings[~test_mask] = 0 # zero all movies we have seen during the training\n",
    "\n",
    "num_users, num_items = generated_ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074628f-62ec-48a6-8eab-adf42e44b3c4",
   "metadata": {},
   "source": [
    "## Finally calculating the MAP@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e07fb7b3-bf2b-4195-82b3-f8e3a7cb6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "# Getting the suggestion\n",
    "_, top_k_indices = torch.topk(generated_ratings, k)\n",
    "# Getting the relevant unseen movies\n",
    "relevant_ratings = ratings_test >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b3bc04-60ef-4003-8fbe-bed1a10c495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7821)\n"
     ]
    }
   ],
   "source": [
    "MAP_sum = 0\n",
    "for user in range(num_users): \n",
    "    ap_sum = 0\n",
    "    counter = 1\n",
    "    for i, idx in enumerate(top_k_indices[user]): # iterate over suggestions\n",
    "        if relevant_ratings[user][idx]: # if this document is relevant\n",
    "            ap_sum += counter / (i+1)\n",
    "            counter += 1\n",
    "    if relevant_ratings[user].sum() == 0: # if we have no relevant documents, then the preicision should be 1\n",
    "        ap_sum = 1\n",
    "    else: # In other case we normalize the result for the user\n",
    "        ap_sum/=min(k, relevant_ratings[user].sum())\n",
    "    MAP_sum += ap_sum\n",
    "print(MAP_sum / num_users)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
